#!/usr/bin/env python
import argparse
import bisect
import re
from pathlib import Path
import shutil

from tqdm import tqdm

# Opening tag anywhere in a line, e.g. "<tbody_1>", "<tbody_1.1>", ...
TBODY_OPEN_RE = re.compile(r"<tbody_([0-9.]+)>", re.IGNORECASE)


def find_leaf_tbody_blocks(text: str):
    """
    Find all <tbody_X>...</tbody_X> ranges and return only the *leaf* ones.

    Returns a list of dicts: {"id": str, "start": int, "end": int},
    sorted by "start".

    Leaf = a tbody range that does NOT contain the start of another tbody.
    """
    open_matches = list(TBODY_OPEN_RE.finditer(text))
    if not open_matches:
        return []

    blocks = []
    for m in open_matches:
        tbody_id = m.group(1)  # e.g. "1", "1.1", "1.1.1"
        start = m.start()

        # closing tag: </tbody_X>
        close_pat = re.compile(r"</tbody_" + re.escape(tbody_id) + r">",
                               re.IGNORECASE)
        close_match = close_pat.search(text, m.end())
        if close_match:
            end = close_match.end()
        else:
            end = len(text)

        blocks.append({"id": tbody_id, "start": start, "end": end})

    # keep only leaf blocks (no other block starts inside)
    leaf_blocks = []
    for i, b in enumerate(blocks):
        has_child = any(
            j != i and blocks[j]["start"] > b["start"] and blocks[j]["start"] < b["end"]
            for j in range(len(blocks))
        )
        if not has_child:
            leaf_blocks.append(b)

    leaf_blocks.sort(key=lambda b: b["start"])
    return leaf_blocks


def build_line_index(text: str):
    """
    Return (lines, line_starts).

    lines: list of lines with keepends=True
    line_starts: list of starting character index of each line in 'text'
    """
    lines = text.splitlines(keepends=True)
    line_starts = []
    pos = 0
    for line in lines:
        line_starts.append(pos)
        pos += len(line)
    return lines, line_starts


def get_context_before_block(
    block_start: int,
    lines,
    line_starts,
    max_lines: int = 3,
    min_line_idx: int = 0,
):
    """
    Get up to 'max_lines' non-empty lines immediately before the block,
    stopping at a blank line or when going above 'min_line_idx'.

    Returns a string.
    """
    # find line index for block_start
    line_idx = bisect.bisect_right(line_starts, block_start) - 1
    ctx_lines = []
    j = line_idx - 1

    count = 0
    while j >= min_line_idx and count < max_lines:
        if lines[j].strip() == "":
            # stop at blank line to not cross paragraphs/sections
            break
        ctx_lines.insert(0, lines[j])
        count += 1
        j -= 1

    return "".join(ctx_lines)


def process_file(src_path: Path, dst_dir: Path) -> None:
    """
    For each source .txt file:

    - Create <stem>_base.txt with the full text but with each leaf tbody
      replaced by a placeholder "__TBODY_PART_n__".
    - For each leaf tbody block, create <stem>_partN.txt containing:
        * first two lines of the file
        * small context just before the tbody (not reusing those two lines)
        * tbody block itself
    """
    text = src_path.read_text(encoding="utf-8", errors="replace")
    blocks = find_leaf_tbody_blocks(text)

    # If no tbody markers: just copy unchanged
    if not blocks:
        dst_path = dst_dir / src_path.name
        shutil.copy2(src_path, dst_path)
        return

    # Build line index and get first two lines
    lines, line_starts = build_line_index(text)
    first_two = "".join(lines[:2]) if len(lines) >= 2 else "".join(lines)
    first_context_idx = 2  # don't let context reuse these first two lines

    # ---- Create base file (everything except tbody content) ----
    segments = []
    last = 0
    for idx, b in enumerate(blocks, start=1):
        segments.append(text[last:b["start"]])
        segments.append(f"__TBODY_PART_{idx}__\n")  # placeholder
        last = b["end"]
    segments.append(text[last:])
    base_text = "".join(segments)

    base_name = f"{src_path.stem}_base{src_path.suffix}"
    (dst_dir / base_name).write_text(base_text, encoding="utf-8")

    # ---- Create one part file per tbody block ----
    for idx, b in enumerate(blocks, start=1):
        context = get_context_before_block(
            b["start"],
            lines,
            line_starts,
            max_lines=3,
            min_line_idx=first_context_idx,
        )
        tbody_text = text[b["start"]:b["end"]]

        part_content = first_two + context + tbody_text

        out_name = f"{src_path.stem}_part{idx}{src_path.suffix}"
        out_path = dst_dir / out_name
        out_path.write_text(part_content, encoding="utf-8")


def main():
    parser = argparse.ArgumentParser(
        description=(
            "Split .txt files on <tbody_X>...</tbody_X> so that:\n"
            "- one *_base.txt file keeps all non-tbody text (with placeholders),\n"
            "- each *_partN.txt contains first two lines + local context + one tbody."
        )
    )
    parser.add_argument("input_dir", help="Folder containing .txt files")
    parser.add_argument("output_dir", help="Folder for output files")
    args = parser.parse_args()

    in_dir = Path(args.input_dir)
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    txt_files = [p for p in in_dir.glob("*.txt") if p.is_file()]

    for src in tqdm(txt_files, desc="Processing .txt files"):
        process_file(src, out_dir)


if __name__ == "__main__":
    main()